{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is similar to neurons. They consists of 4 parts:\n",
    "- Input Values\n",
    "- weights and bias\n",
    "- net sum\n",
    "- activation function \n",
    "So basicly all their inputs are multiplied with their corresponding weights. Then all the weighted inputs are added to the weighted sum. Next the weighted sum is given to the activation function which decides if the perceptron fires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function basicly says under which condition a neuron of the neuronal network fires. \n",
    "The simplest one is the linear activation function. It's just a straight line where the activation is proportional to the input. \n",
    "But there are different activation functions, like the sigmoid function, the tanh function and the ReLu function.\n",
    "The ReLu function is most common today and is basicly 0 for inputs < 0 and linear for inputs > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the error function we can evaluate our neuronal network and we try to minimize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias are very important for neuronal networks. In the network we have a lot of different neurons that all look for different patterns in the input. Of course there are patterns that have an higher impact at our prediction than others. So we have to weight our neurons based on the results we got. So basicly after every epoch we update the weights and bias of our neuronal network based on the result we got.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write definitions of Perceptron, Activation function, Error function, and need for weights and bias. Time limit is 20 mins. After you are done please push it up. This is an individual exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons were invented by Frank Rosenblatt in 1957 as a copy of neurons in the human brain. \n",
    "\n",
    "The basic version - a simple perceptron - consists of a single artificial neuron with adjustable weightings and a threshold value (bias). It can be distinguishhed between single-layer and multi-layer perceptrons (MLP). \n",
    "\n",
    "Perceptron meshes convert an input vector into an output vector. We speak of deep learning if there are more than one hidden layer between input and output. The perceptron algorithm stops updating the weights as soon as all samples are classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different activation functions in a neural network: Linear, Sigmoid, Step. These are normally used in different layers of a model.\n",
    "\n",
    "Step being the simplest with a binary output (discontinous), depends on whether the input meets a specified threshold Î¸. Step is often useful in the last layer of a binary classification network.\n",
    "\n",
    "In the case of a Linear activation function the output unit is the weighted sum of its inputs plus a bias term. It is often used in the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a continous linear activation function, we can define a cost function that we can minimize in order to update our weights. In this case we can define the cost function J(w) as the sum of squared errors (SSE), similar to the cost function that is minimized in ordinary least squares (OLS) linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights determine the degree of influence that the neuron inputs have on the calculation of the later activation. Depending on the sign of the weights, an input can have an inhibitory/hindering or excitatory/stimulating effect. A weight of 0 marks a non-existent connection between two nodes.\n",
    "\n",
    "Bias/threshold does the same as the intercept added in a linear equation. It is an added parameter in the NN which is used to adjust the output along with the weighted sum of the inputs to the neuron. Bias is therefore a constant which helps the model to fit best for the given data. For example if you build a perceptron that transforms celsius into fahrenheit it adds +32 as a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

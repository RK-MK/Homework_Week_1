{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "## Introduction\n",
    "\n",
    "Bayes theorem finds the probability that event A will occur, given that event B already has occured. This is called <strong>conditional probability</strong>.\n",
    "\n",
    "$$P\\left(A\\rvert B\\right)=\\frac{P\\left(B\\rvert A\\right)P(A)}{P(B)}$$\n",
    "\n",
    "The above equation holds if the events can be assumed to be independent of one another.\n",
    "\n",
    "In a machine learning context, Bayes theorem can be rewritten as:\n",
    "\n",
    "$$P\\left(y\\rvert X\\right)=\\frac{P\\left(X\\rvert y\\right)P(y)}{P(X)}$$\n",
    "\n",
    "where $y$ is the (dependent) class variable, i.e. the desired output of the model and $X$ represents the n dimensions of the independent variables (features).\n",
    "$$X = (x_1,x_2,x_3,\\ldots,x_n)$$\n",
    "\n",
    "Expanding the equation, one is left with:\n",
    "\n",
    "$$P\\left(y\\rvert x_1,x_2,x_3,\\ldots,x_n\\right)=\\frac{P\\left(x_1\\rvert y\\right)P\\left(x_2\\rvert y\\right)P\\left(x_3\\rvert y\\right)\\ldots P\\left(x_n\\rvert y\\right)P(y)}{P(x_1)P(x_2)P(x_3)\\ldots P(x_n)}$$\n",
    "\n",
    "The denominator, $P(X)$ is only a scaling factor, as it does not change for all entries of a given dataset. Hence in can be removed. The r.h.s of the above equation can then be rewritten as: \n",
    "\n",
    "$$P\\left(X\\rvert y\\right)=\\prod^d_{\\alpha=1} P(x_{\\alpha}\\rvert y)$$\n",
    "\n",
    "Two important assumptions are made in the Bayes theorem:\n",
    "\n",
    "1. Independence of variables\n",
    "2. Equal importance of variables\n",
    "\n",
    "The assumption of independence is very rarely fulfilled in real life situations. However, often it is fulfilled well enough for Bayes algorithms to deliver decent results.\n",
    "\n",
    "## Bayes Classifier\n",
    "\n",
    "The classifier function $h(x)$ returns the probalities for all possible values of the class variable $y$ for a given set of input variables. To make a prediction, we must find the maximum probality. The mathematical expression for this is:\n",
    "\n",
    "$$h(x)=\\underset{y}{\\arg\\max}P(y\\rvert x)=\\underset{y}{\\arg\\max}\\frac{P(x\\rvert y)P(y)}{z}=\\underset{y}{\\arg\\max}P(y)\\prod^d_{\\alpha=1} P(X_{\\alpha}\\rvert y)=\\underset{y}{\\arg\\max}\\log(P(y))\\prod^d_{\\alpha=1} \\log(P(X_{\\alpha}\\rvert y))$$\n",
    "\n",
    "Depending on the distribution of the predictor and output variable, one distinguishes three different types of Bayes classifiers:\n",
    "\n",
    "* Multinomial: Used for document classification: The features/predictors used by the classifier are the frequency of the words present in the document. The feature vectors are frequency counts of e.g. a specific word in a text.\n",
    "* Bernoulli: Is used when the predictors are boolean values, hence can only take the state True or False\n",
    "* Gaussian: Used when the predictors are continous. It is then assumed, that they are sampled from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the data and assign it to X and y variable\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create at 70/30 split of the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=101)\n",
    "\n",
    "# training the model on training set \n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Testing Gaussian Naive Bayes model with test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print resulting scores\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : Definitions of Perceptron, Activation function, Error function, and need for weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perceptron:</b> A Perceptron was devised as a artificial neuron, which should behave similarly to a neuron in the human brain. It will take Input and check, with a mathematical function, if it should activate or not. As a comparison a neuron would , in simple terms, get some kind of stimulus and decide upon it if it fires off a stimulus or just does not do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Activation function:</b> The activation function denotes the mathematical function I mentioned prior to this, which takes the weighted sum of input data and a bias to check whether the Neuron activates or not. Hence the name. There are a couple of them which are used widely, but they all have in common, that they are non-linear. If they would be we would just end with a linear problem, which we could solve in a more simple manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Error function:</b> After the data has passed our neuron(s) we will have an output of some kind. As we are in supervised learning ( so far ) we know the true value of this output. The Error denotes the spread between the true and output value. We can calculate the Error with several different Error functions, i.e. sum of squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the Error function the mentioned weight and bias come in play. This is also where the neuron(s) \"learn\". By deriving the error function we find the direction where its minimum lies ( as we obviously want to have a small as possible error ). This derivation is then used to update the weight and bias of every single data point and neuron. When this update happens depends. There are a couple of different approaches, ranging from updating them after each sample has passed up to every sample having to pass the neuron(s) before a update happens. <br>\n",
    "Note that the weights are assigned to the data and bias to the neurons, directly in the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Need for weights and bias:</b> To highlight it again, there would be no learning process whatsoever without the weights and bias, so they are essential for the success of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf2] *",
   "language": "python",
   "name": "conda-env-nf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

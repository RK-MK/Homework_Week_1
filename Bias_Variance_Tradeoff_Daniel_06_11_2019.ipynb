{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias-Variance Tradeoff:\n",
    "\n",
    "When modelling the relationship between a predicted variable y and a predicting, regressor variable x, we can write y as a function of x. When doing so for real world data, noise is expected to cloud this relationship. Or in other words all aspects not accounted for by the function f will be covered by these error terms.\n",
    "\n",
    "So when writing the equation y = f(x), noise will be added to the right hand side: y = f(x) + epsilon. These errors epsilon are assumed to be normally distributed with a mean value of 0 and a standard deviation of 1.\n",
    "\n",
    "When now trying to estimate the performance of modelling the function f given a sample training data set, we can compute the Mean Squared Error (MSE), which is the expected value of the squared difference of the real value of f (for the whole population) and our found value of f.\n",
    "\n",
    "When going through all steps disecting this MSE, expected value for the difference of f from the real f, than we see that this is composed of three parts:\n",
    "\n",
    "- 1) the Variance of the difference of f from real f, called variance\n",
    "- 2) the variance of the errors, called bias\n",
    "- 3) the squared difference of the expected values of f and real f, called bias\n",
    "\n",
    "In choosing a model to describe a relationship between y and x, there will be a tradoff with respect to minimizing both variance and bias at the same time, since models with high complexity or capacity (e.g. neural networks with many layers) tend to have a low bias, but a higher variance as opposed to less complex (e.g. linear regression) models which tend to have low variance but a higher bias. Balancing these two opposing features while selecting a certain model to minimze the MSE of the predictor function is called the bias-variance tradeoff. When reducing one of the two, the other tends to go up and vice versa. Finding the right balance between bias and variance is the goal in the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Version:\n",
    "\n",
    "Bias and Variance are measures for evaluating the quality of a model fitted to capture a relation between a target variable and one or more feature variables.\n",
    "\n",
    "Bias is a measure of how bad a model can capture the general behaviour, the structure of the data. Less flexible/sensitive models have higher biases.\n",
    "\n",
    "Variance is a measure of how dependent a model fit is to one specific set of sample data. More flexible models have higher variance as their fit more strongly depends on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

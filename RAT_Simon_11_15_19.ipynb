{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ensemble methods. One for decreasing variance, <b>bagging</b>, one for decreasing bias, <b>boosting</b>, and one for improving predictions, <b>stacking</b>. <br>\n",
    "In <b>bagging</b> we take random samples with replacement, in parallel, taking the average of all estimates for regression and using voting for classification to get a final prediction.<br>\n",
    "For <b>boosting</b> we generally use weaker learners and train them in sequence on a weigthed version of our data, changing the weigths depending on whether the data points were correctly classified or not.<br>\n",
    "<b>Stacking</b> denotes using the result of several learning algorithms as features for so called meta-algorithms whose output is then our final predictor.<br>\n",
    "<br>\n",
    "Regarding use cases the idea of sequential methods hereby is to use the dependence of the base learners, while parallel methods go for the independence. As such we would choose from the above. As a side note, using stable modells as learners in these modells is not very helpful as they will not increase the result by a big margin.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

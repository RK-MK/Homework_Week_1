{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (also called bootstrap aggregating)\n",
    "\n",
    "is an ensemble meta-algorithm technique, typically applied when using random forest, which improves stability and accuracy by reducing variance (and thereby avoiding overfitting) compared to using a single algorithm (decision tree). \n",
    "\n",
    "The training data set will be split in multiple subsets of data with the same lengths as the original training data set and replacements for some observations (bootstramp sample). For each subset of data one model will be fitted. One final prediction/ model will be generated by averaging the output of all single models on the subsets (therefore called: model averaging approach)/(what averaging is for regression models would be voting for classification models). Producing multiple subsets with random data drawn from the orignal training data and then averaging over all produced models will reduce the variance and aviod overfitting.\n",
    "\n",
    "By randomly creating multiple subset of data the bias will increase slightly, but due to a significantly larger decrease of variance the overall result of the final model will be better using bagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "is an ensemble meta-algorithm technique, which reduces the bias (and also the variance) by combining a set of multiple weak learners (= low correlation of model/classifier with actual data) into one strong learner (= well performing model reflecting true relationship of data).\n",
    "\n",
    "When running the weak models the misclassified data will be given a higher weight in order to force the model towards classifying this data correctly. Afterwards these weak models are combined/ added and run again. This process of rerunning the models and reweighting the misclassified data is repeated multiple times (sequencially training the data) to generate a final model, which performace better than the single weak models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "is a methode of ensembled machine learning techniques that combines multiple algorithms (meta-classifier or meta- regressor) in order to obtain a better model/ better predictions (delivering a higher accuracy). \n",
    "\n",
    "The base model is trained on the  original complete training data set, while the meta-models are trained on the output of the base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

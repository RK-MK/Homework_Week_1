{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "A perceptron is a single-layer neural network in which a \"neuron\" (processing unit) generates an output from some inputs. It can be used as a supervised learning algorithm to solve binary classification problems.\n",
    "\n",
    "The \"neuron\" in this perceptron takes some inputs (x1...xm) and calculates a weighted sum of these inputs with the weights w1...w0 and a bias b:<br>\n",
    "z = w1 \\* x1 + ... + wm \\* xm + b\n",
    "\n",
    "\n",
    "This weighted sum (z) is then transformed by an \"activation function\" g(z) to generate the output of the neuron. For example, a simple activation function is called unit step function and returns 1 if zâ‰¥0 and -1 otherwise. In a classification problem, this output can denote if a sample belongs to one class (1) or the other (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "An activation function in a neural network transforms the number calculated by the neuron (i.e. the weighted sum of the previous layer's inputs) in a non-linear fashion into some output. This non-linear activation function is needed since a neural network without activation functions would only generate outputs which are linear functions of the inputs, hence it would not be more powerful than a single neuron (linear function). \n",
    "\n",
    "There are different activation functions that are used in neural networks, like ReLU (Rectified Linear Unit), Sigmoid, Step Unit Function and Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function\n",
    "\n",
    "The error function quantifies the difference (often in form of the mean squared error) between predicted and true values. Hence, it quantifies how \"off/wrong\" a network's prediction is compared to the true/desired outcome and is then used to change the parameters of the network (weights, biases) to improve the networks output during the training process, i.e. to lower the prediction error until it gets minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "The weights and biases in a neural network are the parameters that determine the activation of a given neuron (together with the inputs the neuron receives). \n",
    "\n",
    "The \"weight\" is basically the equivalent of the \"synaptic strength/weight\" in a biological neural network: The higher the weight between two neurons (nodes), the stronger the neuron gets activated by the input from the previous layer's neuron. \n",
    "\n",
    "\n",
    "The bias is basically an additional weight that requires no input (or rather a fixed input of 1) and plays a role in determining how active the neuron is with zero input or how easily it is triggered by non-zero input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

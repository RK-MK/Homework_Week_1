{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff in Machine Learning (ML) describes a trade-off between model flexibility and model generizability. As with other tradeoffs, we can say that both a model with \"too much bias\" and a model with \"too much variance\" are not good models - the right answer lies somewhere in the middle and we should aim for a model that trades off both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do **bias** and **variance** mean in ML? I will try to give a (non-mathematical) description, using a simple linear regression example (but see below for a short summary). Consider again our King County House Price dataset. We had a feature in this dataset - the house grade - which was predictive for the house sales price. However, if we tried to model a linear relationship between grade price, we could see that the linear fit (the straight line) systematically underestimated the price for houses with a very high grade. This means, the model's predictions were **biased** for these data towards lower price values compared to the true prices. The model **underfit** the data and did not completely capture the true relationship between grade and price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome this bias, we could try increase the models complexity (by adding extra degrees of freedom to the model, i.e. extra model parameters), which makes the model more flexible such that it fits the price more accurately for very high-graded houses. Let's say we first \"shoot over the goal\" and decide to add three extra parameters to the model: one extra regression parameter for the grade squared, one for the grade to the power of 3 and one for the grade to the power of 4. When we now fit the model to the data, it follows the points in the scatterplot very closely, also for high-graded houses. The model is now very flexible, but has another problem: hight **variance**, meaning it **overfits** the data and does not generalize to new datasets. We can see what the problem is when we try to predict the price for new (test) data based on this overfitted model: The model does not capture the pattern seen in the new dataset very well. This is because it was \"overtrained\" on the training set and picked up some random variation in the training data, which is not seen in the new test data. As we want to use our model for prediction, we cannot accept this high degree of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we do? We aim for the \"golden middle solution\" - a model with low bias and low variance, which does not underfit nor overfit the data. A model, which shows a good fit to the training data (low bias), but also good predictive performance (low variance) for new test data. In this example, we could achieve this trade-off by fitting a regression model with two regression parameters (one for grade, one for grade squared). This captures the curvature in the relationship between grade and price seen in the training data very well, but does not \"get distracted\" (does not capture) random artifacts (i.e. sampling noise) in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary:\n",
    "- **Bias** (the problem of underfitting): \n",
    "    - decreases with increasing model complexity\n",
    "    - a biased model does not correctly capture the systematic pattern in the data\n",
    "    \n",
    "    \n",
    "- **Variance** (the problem of overfitting): \n",
    "    - increases with increasing model complexity\n",
    "    - a model with high variance does also capture random noise in the data and is not generalizable to new data\n",
    "    \n",
    "    \n",
    "- **Bias/Variance Tradeoff**: \n",
    "    - find the \"right\" degree of model complexity for the problem at hand\n",
    "    - a good trade-off will ensure that the model captures rather true patterns in the data than noise and is generalizable to new datasets/environments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
